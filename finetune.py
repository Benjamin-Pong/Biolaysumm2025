# -*- coding: utf-8 -*-
""" Finetunellama3-8b-instruct.ipynb
Reference: Unsloth
Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TrkATSvgKWaWzJ4jghB8gI5yVGXAVDuY
"""

!pip install unsloth
from unsloth import FastLanguageModel
from transformers import AutoTokenizer
import torch
max_seq_length = 4096
dtype =  None
load_in_4bit = True #quantization - decrease memory usage

model, Tokenizer = FastLanguageModel.from_pretrained(
    model_name = "meta-llama/Meta-Llama-3-8B-Instruct",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)


# Apply LoRA adapter - weights are being updated here
model = FastLanguageModel.get_peft_model(
    model,
    r = 10,
    lora_alpha = 16,
    target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],  # Adding more target modules
    lora_dropout = 0.1,  # Adding small dropout for regularization
    bias = "none",
    use_gradient_checkpointing = True,
    random_state = 3407,
    use_rslora = False,
    loftq_config = None,
)
#Add EOS token to avoid infinite loop
EOS = Tokenizer.eos_token_id

######  Load the data that will be used for finetuning #######################
import json
from google.colab import drive
from datasets import Dataset
drive.mount('/content/drive')

config = '3'
###### load training_data - the 200 summaries generated from preprocessed data ########
train_data_repo = f"path/to/file"

with open(train_data_repo) as f:
    train_data = json.load(f)

train_data = Dataset.from_list(train_data)
print(len(train_data))

def truncate_to_token_limit(article, token_limit=4096):

    # Initialize tokenizer
    #tokenizer = AutoTokenizer.from_pretrained(model_id)

    # Tokenize the text
    tokens = Tokenizer.encode(article)
    # Check if truncation is needed
    if len(tokens) <= token_limit:
        return article

    # If truncation is needed, keep only the first 'token_limit' tokens
    truncated_tokens = tokens[:token_limit]

    # Convert back to text
    truncated_text = Tokenizer.decode(truncated_tokens, skip_special_tokens=True)

    return truncated_text


###### process data - data has to be in a certain format for training #####

def formatting_function(data_sample):
    article = truncate_to_token_limit(data_sample['preprocessed article']) #processed (2-3) or unprocessed (1)
    summary = data_sample['gold summary'] #gold summary
    prompt = f'''
      <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a chatbot with expertise in summarizing documents. <|eot_id|>
      <|start_header_id|>user<|end_header_id|>
      Provide a lay summary of this article: {article} <|eot_id|>
      <|start_header_id|>assistant<|end_header_id|>
      Lay Summary:
      {summary} <|eot_id|>
      '''
    return {'text': prompt}

#Special tokens for llama3.1
#https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/


#formatted = formatting_function(train_data)

#tokenized_train_dataset = tokenize_function(train_data)
#tokenized_val_dataset = tokenize_function(val_data)
train_data = train_data.map(formatting_function)

#val_data = val_data.map(formatting_function)

print(train_data[0]['text'])


'''
[{'article':......., 'gold':......, 'text': prompt}]
'''

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
from huggingface_hub import login

# https://unsloth.ai/blog/gradient 0 fix gradient
torch.cuda.empty_cache()
output_directory = "./llama3-8b-instruct-finetuning"


sample_size = '1000'
## name of your model #
repo_name = f"llama_4.{config}_{sample_size}"

trainer = SFTTrainer(
    model = model,
    tokenizer = Tokenizer,
    train_dataset = train_data,
    #eval_dataset = val_data,
    dataset_text_field = "text",
    #formatting_func = formatting_func,
    max_seq_length = max_seq_length,

    args = TrainingArguments(
        output_dir = output_directory,
        per_device_train_batch_size = 4,
        gradient_accumulation_steps = 2,  #effective batch size: 4 x 2 #forward to compute activation, background
        learning_rate = 2e-5, #10^-5
        lr_scheduler_type = "linear", #10^5 --> 0
        #regularization
        #weight_decay = 0.01
        fp16 = False,
        bf16 = True,
        num_train_epochs=3, #100 samples, 5 , 20 samples

        # Checkpointing and validation
        #eval_strategy="no",               # Updated from evaluation_strategy
        #eval_steps=5,                      # Evaluate every 50 steps
        save_steps=5,                      # Save every 50 steps
        save_total_limit=1,                  # Keep only the 5 most recent checkpoints
        #load_best_model_at_end=True,
        #metric_for_best_model="eval_loss",   # Might need another check

        # Logging parameters
        logging_dir="./logs",                # Directory for logs
        logging_steps=5,                    # Log every 5 steps
        logging_first_step=True,             # Log the first step

        # Hugging Face Hub parameters
        push_to_hub=True,                    # Push model to Hugging Face Hub
        hub_model_id=repo_name,
        hub_strategy="every_save",           # Push every time we save a checkpoint
        hub_private_repo=False,              # Make the repository public


    )
)


#trainer.train()
from unsloth import unsloth_train
# unsloth_train fixes gradient_accumulation_steps
trainer_stats = unsloth_train(trainer)

print("Starting training...")

print (f'final ')
print("Evaluating final model...")
#eval_results = trainer.evaluate()

# Print evaluation results (with proper error handling)
'''
print("Evaluation results:")
if eval_results and "eval_loss" in eval_results:
    print("Final eval_loss:", eval_results["eval_loss"])
else:
    print("Warning: No eval_loss found in evaluation results.")
    print("Available keys:", eval_results.keys() if eval_results else "None")
'''
# Push to Hugging Face Hub
print("Pushing final model to Hugging Face Hub...")
trainer.push_to_hub("fine-tuned llama model")

print("Fine-tuned llama model is now available at:", repo_name)
