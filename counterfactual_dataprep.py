# -*- coding: utf-8 -*-
"""Counterfactual Data Augmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y5tDFWBafqbi7oSFeThN3haHdw1T8bpS
"""

'''
This script prepares counterfactual data.

Data-base of biomedical NER
--> According to Indices
[{0: {'TAG1': [list of entities], 'TAG2': [List of Entities]}},  {1: {'TAG1': [list of entities], 'TAG2': [List of Entities]}}]



Gold Summary
Control of the chronic form of sleeping sickness or gambiense human African trypanosomiasis ( HAT ) consists of accurate diagnosis followed by treatment . We aim to replace the native variant surface glycoprotein ( VSG ) parasite antigens that are presently used in most antibody detection tests with peptides that can be synthesised in vitro . Antibodies recognising VSG were purified from HAT patient sera and were used to select phage-expressed peptides that mimic VSG epitopes from a Ph . D . -12 phage display library . The diagnostic potential of the corresponding synthetic peptides was demonstrated in indirect ELISA with sera from HAT patients and endemic negative controls . We proved that diagnostic mimotopes for T . b . gambiense VSGs can be selected by phage display technology, using polyclonal human antibodies .

Every gold summary has an index

for every gold summary, replace all entities with randomly selected entities from another randomly selected gold summary. To

And then prompt the LLM to generate a wrong summary.

Goal is to induce errors --> improve factuality
'''

'''
# Use a pipeline as a high-level helper
from transformers import pipeline
# Load model directly
from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification

Tokenizer = AutoTokenizer.from_pretrained("dmis-lab/biobert-v1.1")
MODEL = AutoModelForTokenClassification.from_pretrained("dmis-lab/biobert-v1.1")



text = "Control of the chronic form of sleeping sickness or gambiense human African trypanosomiasis ( HAT ) consists of accurate diagnosis followed by treatment . We aim to replace the native variant surface glycoprotein ( VSG ) parasite antigens that are presently used in most antibody detection tests with peptides that can be synthesised in vitro . Antibodies recognising VSG were purified from HAT patient sera and were used to select phage-expressed peptides that mimic VSG epitopes from a Ph . D . -12 phage display library . The diagnostic potential of the corresponding synthetic peptides was demonstrated in indirect ELISA with sera from HAT patients and endemic negative controls . We proved that diagnostic mimotopes for T . b . gambiense VSGs can be selected by phage display technology, using polyclonal human antibodies ."
ner_pipeline = pipeline("ner", model=MODEL, tokenizer=Tokenizer)
labels = MODEL.config.id2label

# Print the labels
print(labels)
entities = ner_pipeline(text)

# Print the results
for entity in entities:
    print(f"Entity: {entity['word']}")
    print(f"Type: {entity['entity']}")
    print(f"Confidence: {entity['score']:.4f}")
    print("-" * 30)
'''

!pip install stanza
import stanza

'''
stanza.download('en', processors = {'ner': 'JNLPBA' })

text = "Control of the chronic form of sleeping sickness or gambiense human African trypanosomiasis ( HAT ) consists of accurate diagnosis followed by treatment . We aim to replace the native variant surface glycoprotein ( VSG ) parasite antigens that are presently used in most antibody detection tests with peptides that can be synthesised in vitro . Antibodies recognising VSG were purified from HAT patient sera and were used to select phage-expressed peptides that mimic VSG epitopes from a Ph . D . -12 phage display library . The diagnostic potential of the corresponding synthetic peptides was demonstrated in indirect ELISA with sera from HAT patients and endemic negative controls . We proved that diagnostic mimotopes for T . b . gambiense VSGs can be selected by phage display technology, using polyclonal human antibodies ."
nlp = stanza.Pipeline('en', processors = {'ner': 'JNLPBA'})
doc = nlp(text)
#print(doc)
print(doc.entities)
for ent in doc.entities:
    print(f"{ent.text}\t{ent.type}")
'''

###### Main code starts here BERN 2 ###############
import requests
import json

def get_entities(text, url="http://bern2.korea.ac.kr/plain"):
    '''
    This function retrieves all unique entities for a paragraph of text using BERN2's API.
    '''
    try:
        response = requests.post(url, json={'text': text})
        response.raise_for_status()  # Raise an exception for bad status codes
        return response.json()
    except requests.exceptions.JSONDecodeError as e:
        print(f"Error decoding JSON: {e}")
        print(f"Response content: {response.content}")  # Print the raw response for debugging
        return {'annotations': []} # Return an empty list to proceed
    except requests.exceptions.RequestException as e:
        print(f"Request error: {e}")
        # Handle other request errors
        return {'annotations': []}

import re

def augment_with_NER(text, map):
  '''
  This function replaces every entity mention with its corresponding entity category, enclosed in '*'.
  '''
  print(text)
  for key in map:
    print(key)
    replacements = []  # Store replacements as (start_index, end_index, replacement_word) tuples
    for match in re.finditer(r'\b' + re.escape(key) + r'\b', text):
      start_idx = match.start()
      end_idx = match.end()
      print(f"The word '{key}' is found at span: ({start_idx}, {end_idx})")
      replacements.append((start_idx, end_idx, map[key]))  # Store replacement details
    replacements.sort(reverse=True)

    # replace
    for start_idx, end_idx, replacement_word in replacements:
        text = text[:start_idx] + "*"+ replacement_word + "*" + text[end_idx:]

  return text

from huggingface_hub import login
from transformers import pipeline
import transformers
import torch
model_id = 'meta-llama/Meta-Llama-3-8B-Instruct'
pipeline = transformers.pipeline("text-generation", model=model_id, model_kwargs={"torch_dtype": torch.bfloat16}, device_map="auto")

def LLM_augmentation(pipeline, text):
    messages = [
        {"role": "system", "content": "You are a chatbot with knowledge in medical terms and their definitions in context."},
        {"role": "user", "content": f'The following text contains words enclosed in \'*\' These words are categories for biomedical entities. Replace the words with randomly chosen biomedical entities from your wealth of knowledge, and then enumerate a list of the replacements. {text}'},
      ]

    outputs = pipeline(
        messages,
    )
    response = outputs[0]["generated_text"][-1]['content']
    print(f"response: {response}")
    #clean up predicted summary
    #response = data_sample['predicted_summary']['content']
    if ":" in response:
      cleaned_response = response.split(":",1)[1]
    else:
      cleaned_response = response

    cleaned_response =  cleaned_response.replace("\n\n", " ")
    print(f"cleaned: {cleaned_response}")
    return cleaned_response

from google.colab import drive
drive.mount('/content/drive')

import random
if __name__ == '__main__':
    '''
    Randomly select 250 data points from the 1000 samples
    '''
    import json
    repo = f"/content/drive/MyDrive/eLife_PLOS_samples/preprocessed_1000/3.json"
    output_repo = f"/content/drive/MyDrive/eLife_PLOS_samples/Data Augmentation/mixture_500.json"

    with open(repo, 'r') as f:
        data = json.load(f)

    random.shuffle(data)
    print(len(data))
    original_data = data[250:]
    output = original_data
    output = []
    data_to_be_augmented = data[:251]

    for data_o in data_to_be_augmented:
      article = data_o['preprocessed article']
      summary = "'"+ data_o['gold summary'] + "'"
      print(summary)
      NER = get_entities(summary ,url="http://bern2.korea.ac.kr/plain")
      #print(NER)
      mention_entity_map = {}
      for item in NER['annotations']:
        #print(f"{item['mention']}, {item['obj']}, {item['span']}")
        mention_entity_map[item['mention']] = item['obj']
      #print(mention_entity_map)

      #augment the gold summ with NER
      NER_augmented_gold_summ = augment_with_NER(summary, mention_entity_map)
      print(NER_augmented_gold_summ)

      #LLM_augmented_NER
      LLM_augmented_NER = LLM_augmentation(pipeline, NER_augmented_gold_summ)
      print(LLM_augmented_NER)

      output.append({'preprocessed_article': article, 'gold_summary': summary, 'NER_augmented': NER_augmented_gold_summ, 'LLM_augmented': LLM_augmented_NER})

    with open(output_repo, 'w') as f:
      json.dump(output, f)