# -*- coding: utf-8 -*-
"""NER_Hybrid_post_edit_augmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mgm5y95dOiVEp1Bpz9ieEf9VTRE0kP_9
"""

'''
LLM-based definition modelling:
This code is used to replace all words with * to mark them out as biomedical terms that require definition modelling
'''

!pip install scispacy
!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz
!pip install spacy==3.7.5
!pip install numpy==1.26.4
!pip install bitsandbytes


import transformers
import torch
from google.colab import drive
drive.mount('/content/drive')

from huggingface_hub import login
import re
from transformers import AutoTokenizer



import json


import scispacy
from scispacy.linking import EntityLinker
import spacy


nlp = spacy.load("en_core_sci_scibert")
#add entity linker (i.e UMLS database)
nlp.add_pipe("scispacy_linker", config={"linker_name": "umls"})



model_id = "Benjaminpwh/llama_1.3_1000"


pipeline = transformers.pipeline("text-generation", model=model_id, model_kwargs={"torch_dtype": torch.bfloat16}, device_map="auto")


def clean_text(text):
    text = text.replace(' . ', '. ').replace(' , ', ', ')
    text = text.replace('\n\n', ' ')
    text = re.sub(r'\s\[.*?\]', "", text)
    text = re.sub(r'(\(\s([^()]*\s\,\s)*[^()]*\s\))', "", text)
    return text


def truncate_to_token_limit(article, token_limit):

    # Initialize tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    # Tokenize the text
    tokens = tokenizer.encode(article)
    # Check if truncation is needed
    if len(tokens) <= token_limit:
        print('no truncation needed')
        return article
    else:
        print('truncation needed')
    # If truncation is needed, keep only the first 'token_limit' tokens
    truncated_tokens = tokens[:token_limit]

    # Convert back to text
    truncated_text = tokenizer.decode(truncated_tokens, skip_special_tokens=True)

    print(f"Original text length: {len(tokens)}")
    print(f"Truncated text length: {len(truncated_tokens)}")
    return truncated_text

def definition_modelling(word):
  messages = [
      {"role": "system",
       "content": '''You are an expert who can provide informative and lay definitions to biomedical terms.'''},
      {"role": "user", "content": f'Provide only the definition of the biomedical term: {word}'},
    ]

  outputs = pipeline(
      messages,
      max_new_tokens=70,
  )
  response = outputs[0]["generated_text"][-1]['content']
  #print(f"response: {response}")
  #clean up predicted summary
  #response = data_sample['predicted_summary']['content']
  if ":" in response:
    cleaned_response = response.split(":",1)[1]
  else:
    cleaned_response = response

  cleaned_response =  cleaned_response.replace("\n\n", " ")
  print(f"cleaned: {cleaned_response}")
  return cleaned_response

import random
if __name__ == '__main__':
  repo_elife = f"/content/drive/MyDrive/biolaysumm/test/1.1/elife.txt"
  repo_plos = f"/content/drive/MyDrive/biolaysumm/test/1.1/plos.txt"
  output_elife = f"/content/drive/MyDrive/biolaysumm/test/1.2/post-editing-hybrid/elife_.json"
  output_plos = f"/content/drive/MyDrive/biolaysumm/test/1.2/post-editing-hybrid/plos_.json"

  linker = nlp.get_pipe("scispacy_linker")
  fmt_str = "{:<20}| {:<10}| {:<32}| {:<20}"
  outputE = []
  outputP = []
  all_term_dictionaries = []
  with open(repo_elife, 'r') as f, open(repo_plos, 'r') as g:
    for article in f:
      article = article.strip()
      NER = nlp(article).ents
      print(NER)
      #print(NER)
      term_dictionary = {}
      database_used = {'llm':[], 'ontology':[]}
      for entity in NER:
        print(entity)

        #get scispacy definitions
        if entity._.kb_ents and entity._.kb_ents[0]:
          first_cuid = entity._.kb_ents[0][0]
          kb_entry = linker.kb.cui_to_entity[first_cuid]
        #check if the database contains the words. If yes, use it, and if no, use LLMs
        if kb_entry.definition:
          definition = kb_entry.definition[0:]
          print(f'umls: {definition}')
          database_used['ontology'].append(entity.text)
        else:
          #llm definition
          definition = definition_modelling(entity.text)
          print(f'llm: {definition}')
          database_used['llm'].append(entity.text)
        term_dictionary[entity.text] = definition
      print(term_dictionary)
      outputE.append({'preprocessed_article': article, 'term_dictionary': term_dictionary})

    for article in g:
      article = article.strip()

      NER = nlp(article).ents

      print(NER)
      #print(NER)
      term_dictionary = {}
      database_used = {'llm':[], 'ontology':[]}
      for entity in NER:
        definition = definition_modelling(entity.text)
        print(entity.text)
        print(definition)
        #get definitions
        #get scispacy definitions
        if entity._.kb_ents and entity._.kb_ents[0]:
          first_cuid = entity._.kb_ents[0][0]
          kb_entry = linker.kb.cui_to_entity[first_cuid]
        #check if the database contains the words. If yes, use it, and if no, use LLM to get definitions
        if kb_entry.definition:
          definition = kb_entry.definition[0:]
          print(f'umls: {definition}')
          database_used['ontology'].append(entity.text)
        else:
          #llm definition
          definition = definition_modelling(entity.text)
          print(f'llm: {definition}')
          database_used['llm'].append(entity.text)

        term_dictionary[entity.text] = definition
      print(term_dictionary)
      outputP.append({'preprocessed_article': article, 'term_dictionary': term_dictionary, 'database': database_used})


  with open(output_elife, 'w') as f:
    json.dump(outputE, f)

  with open(output_plos, 'w') as g:
    json.dump(outputP, g)

print(outputP)

"""Some limitations of our method is that while it singles out medical terms, it also singles out medical terms that lay people are familiar with. For example, "blood" is something we all know, we dont need the definition of blood to understand the article."""