# -*- coding: utf-8 -*-
"""evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tMrQiafafcJ1nDsWi0O2OCuIBSyaBwfp
"""

!pip install numpy

!pip install bert-score py-readability-metrics nltk pytorch-ignite
!python -m nltk.downloader punkt
!pip install accelerate==0.31.0 huggingface_hub summac
!pip install -q lens-metric==0.2.0
!pip install rouge_score
!python --version

!pip install torch==1.12.1


from google.colab import drive
drive.mount('/content/drive')
import json

##### Load Data ##########
config='2'
output = f"path/file"
#output = f"/content/drive/MyDrive/llama_1.3_1000_llm-def.json"

with open(output) as o:
  output  = json.load(o)

print(output[0].keys())

########### Factuality Scores ###########
from summac.model_summac import SummaCZS, SummaCConv
import nltk

nltk.download('punkt_tab')

#model = SummaCZS(granularity="sentence", model_name="vitc", device="cuda")
model = SummaCConv(models=["vitc"], bins='percentile', granularity="sentence", nli_labels="e", device="cuda", start_file="default", agg="mean")

all_scores = []

for data_o in output:
  predicted_summ = data_o['generated_summary']
  print(predicted_summ)

  doc = data_o['article']
  print(doc)
  score1 = model.score([doc], [predicted_summ]) # Order is document, summary
  print(score1)
  all_scores.append(score1['scores'])
#print("Summary Score 1 consistency: %.3f" % (score1["scores"][0]))

#average
import torch
import numpy as np
print (all_scores)
print(f'mean: {np.mean(all_scores)}')

data_array = np.array(all_scores)
print(data_array)
mean = np.mean(data_array)
print(mean)

# READABILITY SCORES
import numpy as np
from readability import Readability
import nltk
nltk.download('punkt_tab')

# Readability only works with text over 100 words
# Run the block at the top with test_summ or provide your own text at this point.

config_flesch_kincaid = []
config_dale_chall = []
config_coleman_liau = []

for data_o in output:

  summ = data_o['generated_summary']

  if len(summ.split()) >= 100:

    r = Readability(summ)

    config_flesch_kincaid.append(r.flesch_kincaid().score)
    config_dale_chall.append(r.dale_chall().score)
    config_coleman_liau.append(r.coleman_liau().score)

  else:
    print("Skipping summaries with less than 100 words.") # Optional: Print a message when skipping


print(np.mean(config_flesch_kincaid))


print(np.mean(config_dale_chall))


print(np.mean(config_coleman_liau))



# BERT SCORE

from bert_score import score

all_original_F1 = []
for data_o in output:

  predicted_summ = [data_o['generated_summary']]
  gold_summ = [data_o['gold summary']]

  #list have to be the same length, strings do not need to be the same length
  #test_summ and test_ref have to be a list

  P, R, F1 = score(predicted_summ, gold_summ, lang="en") # Precision, Recall, and F1 scores
  print(F1)
  all_original_F1.append(F1)

import torch
import numpy as np

print(f'mean bert: {np.mean(all_original_F1)}')


##### using official rouge package ########


import numpy as np
from rouge_score import rouge_scorer

scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], \
                                    use_stemmer=True, split_summaries=True)
scores = []

for data in output:
  ref = data['gold summary']
  pred = data['generated_summary']
  scores.append(scorer.score(pred, ref))
print(np.mean([s['rouge1'].fmeasure for s in scores]))
print(np.mean([s['rouge2'].fmeasure for s in scores]))
print(np.mean([s['rougeLsum'].fmeasure for s in scores]))

### LENS ###


from lens import download_model, LENS

# Make sure output is actually loaded above
data_sample = output # On 10 pieces of data this took ~45 minutes on a T4

original = [o['article'] for o in data_sample]
gold = [o['gold summary'] for o in data_sample]
generated = [o['generated_summary'] for o in data_sample]

# Use GPU for evaluation, if it is available
import torch
DEVICES = [0] if torch.cuda.is_available() else None

# Load LENS
lens_path = download_model("davidheineman/lens")
lens = LENS(lens_path, rescale=True)

# Quiet PyTorch Lightning logger
import logging, warnings
logging.getLogger("pytorch_lightning.utilities.rank_zero").setLevel(logging.ERROR)
warnings.filterwarnings("ignore")

# Evaluate the simplification with LENS
scores = lens.score(original, generated, gold, batch_size=8, devices=DEVICES)

print(f'LENS score: {scores}')


### AlignScore ######

from alignscore import AlignScore

data_sample = output[:10]
preds = [o['generated_summary'] for o in data_sample]
docs = [o['article'] for o in data_sample]
alignscorer = AlignScore(model='roberta-base', batch_size=16, device='cuda:0',ckpt_path='./AlignScore/AlignScore-base.ckpt', evaluation_mode='nli_sp')
scores = alignscorer.score(contexts=docs, claims=preds)

print(f'AlignScore: {scores}')

