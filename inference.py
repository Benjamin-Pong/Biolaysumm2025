# -*- coding: utf-8 -*-
"""Copy of Summarization_Llama3-8b.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BUWcrfWzM0ksbKF4fjTt1qZ2qGQaOA4B
"""


from huggingface_hub import notebook_login

notebook_login()

from google.colab import drive
drive.mount('/content/drive')
import json

import transformers

import torch

model_id = "meta-llama/Llama-3.1-8B-Instruct"

pipeline = transformers.pipeline("text-generation", model=model_id, model_kwargs={"torch_dtype": torch.bfloat16}, device_map="auto")
from transformers import AutoTokenizer
import numpy as np

def truncate_to_token_limit(article, token_limit):

    # Initialize tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    # Tokenize the text
    tokens = tokenizer.encode(article)
    # Check if truncation is needed
    if len(tokens) <= token_limit:
        return article

    # If truncation is needed, keep only the first 'token_limit' tokens
    truncated_tokens = tokens[:token_limit]

    # Convert back to text
    truncated_text = tokenizer.decode(truncated_tokens, skip_special_tokens=True)

    return truncated_text

def summarize(pipeline, all_data):
  all_predicted_and_gold = []
  for data in all_data:

    gold_summary=data['gold_summary']
    article = data['preprocessed article']
    truncated_article = truncate_to_token_limit(article, token_limit=4096)
    messages = [
      {"role": "system", "content": "You are a chatbot with expertise in summarizing documents"},
      {"role": "user", "content": f'Provide a lay summary of this abstract: {truncated_article}'},
    ]

    outputs = pipeline(
        messages,

        max_new_tokens=256,
    )
    predicted_summary = outputs[0]["generated_text"][-1]
    print(predicted_summary)
    all_predicted_and_gold.append({'predicted_summary': predicted_summary, 'gold_summary': gold_summary})
  return all_predicted_and_gold

if __name__ == "__main__":
  repo = f"path/to/input"
  output_repo = f"/path/to/output"

  with open(repo, "r") as f:
    data = json.load(f)

  all_predicted_and_gold = summarize(pipeline, data)
  with open(output_repo, "w") as f:
    json.dump(all_predicted_and_gold, f)







