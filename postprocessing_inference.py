# -*- coding: utf-8 -*-
"""Postprocessing_inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AygOrNiiVDTH6bwT4K0feNQ4HI-ytX5_
"""

!nvidia-smi
!pip install bitsandbytes

from huggingface_hub import login


!pip install nltk
import nltk
from nltk.corpus import brown
from nltk.corpus import inaugural
from nltk.corpus import treebank
nltk.download('treebank')
nltk.download('inaugural')
nltk.download('brown')
nltk.download('stopwords')

from google.colab import drive
drive.mount('/content/drive')
import json

import transformers
import torch

model_id = "Benjaminpwh/llama_1.3_1000"


pipeline = transformers.pipeline("text-generation", model=model_id, model_kwargs={"torch_dtype": torch.bfloat16}, device_map="auto")



from transformers import AutoTokenizer
import numpy as np

def truncate_to_token_limit(article, token_limit):

    # Initialize tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    # Tokenize the text
    tokens = tokenizer.encode(article)
    # Check if truncation is needed
    if len(tokens) <= token_limit:
        print('no truncation needed')
        return article
    else:
        print('truncation needed')
    # If truncation is needed, keep only the first 'token_limit' tokens
    truncated_tokens = tokens[:token_limit]

    # Convert back to text
    truncated_text = tokenizer.decode(truncated_tokens, skip_special_tokens=True)

    print(f"Original text length: {len(tokens)}")
    print(f"Truncated text length: {len(truncated_tokens)}")
    return truncated_text

def summarize(pipeline, all_data):
  all_predicted_and_gold = []
  reconstructed_data = []
  for data in all_data:

    #print(gold_summary)
    summary = data['preprocessed_article'] #editing the summary from subtask1.1
    print(summary)
    term_dictionary = data['term_dictionary']
    first_10_entries = dict(list(term_dictionary.items())[:10])

    #print(f'combined: {combined}')
    #truncated_article = truncate_to_token_limit(article, token_limit=4096)
    #print(truncated_article)
    messages = [
    {
        "role": "system",
        "content": "You are an expert biomedical editor skilled at simplifying complex medical terms for a lay audience. Use the provided dictionary to replace technical terms with their lay definitions while preserving the original meaning."
    },
    {
        "role": "user",
        "content": f"""
        **Biomedical Lay Definitions Dictionary:**
        {term_dictionary}

        **Task:**
        - Read the following summary: {summary}
        - Replace all technical terms in the summary with their lay definitions from the dictionary.
        - Do not add or remove key information.
        - If a term isn't in the dictionary, retain the original term.

        **Return only the paraphrased summary in one line, without any commentary**
        """
    }
]

    outputs = pipeline(
        messages,
        max_new_tokens=256,
    )
    response = outputs[0]["generated_text"][-1]['content']
    print(f"response: {response}")
    #clean up predicted summary
    #response = data_sample['predicted_summary']['content']
    if ":" in response:
      cleaned_response = response.split(":",1)[1]
    else:
      cleaned_response = response


    cleaned_response =  cleaned_response.replace("\n\n", " ")
    cleaned_response = cleaned_response.strip()
    cleaned_response = cleaned_response.lstrip('\n')
    cleaned_response = cleaned_response.replace("**", "")
    cleaned_response = cleaned_response.replace("\n", " ").strip()
    if cleaned_response == "" or cleaned_response == " " or cleaned_response is False:
      cleaned_response = summary
    print(f"cleaned: {cleaned_response}")
    all_predicted_and_gold.append(cleaned_response)

  return all_predicted_and_gold

if __name__ == "__main__":

  output_elife = f"/content/drive/MyDrive/biolaysumm/test/1.2/post-editing-hybrid/elife_.json"
  output_plos = f"/content/drive/MyDrive/biolaysumm/test/1.2/post-editing-hybrid/plos_.json"
  output_plos_o=f"/content/drive/MyDrive/biolaysumm/test/1.2/post-editing-hybrid/plos.txt"
  output_elife_o=f"/content/drive/MyDrive/biolaysumm/test/1.2/post-editing-hybrid/elife.txt"

  with open(output_plos, "r") as f:
    PLOS = json.load(f)

  with open(output_elife, "r") as f:
    eLife = json.load(f)
  print(PLOS[0])
  print(eLife[0])
  all_predicted_and_gold_PLOS = summarize(pipeline, PLOS)
  all_predicted_and_gold_eLife = summarize(pipeline, eLife)

  with open(output_plos_o, "w") as f:
    for item in all_predicted_and_gold_PLOS:
      f.write(item + "\n")

  with open(output_elife_o, "w") as g:
    for item in all_predicted_and_gold_eLife:
      g.write(item + "\n")


full_plos = f'/content/drive/MyDrive/eLife_PLOS_samples/Data Augmentation/for case study/PLOS_test_ontology_augmented_full.json'
  full_eLife = f'/content/drive/MyDrive/eLife_PLOS_samples/Data Augmentation/for case study/eLife_test_ontology_augmented_full.json'

  with open(output_plos_o, "r") as g:
     line_count = len(g.readlines())
     print(line_count)

  with open(output_elife_o, "r") as g:
     line_count = len(g.readlines())
     print(line_count)

  with open(full_plos, "w") as f:
    json.dump(recon_PLOS, f)

  with open(full_eLife, "w") as g:
    json.dump(recon_elife, g)